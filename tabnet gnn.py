# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qCp7QimxZHAq7NCBgDoCYtj9l8aXUPNX
"""

!pip install pytorch_tabnet

import pandas as pd
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from pytorch_tabnet.tab_model import TabNetClassifier
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score

df = pd.read_csv('/content/1subset_10percent train.csv')
df.head()

X = df.drop("Class_Label", axis=1)          #Data
y = df["Class_Label"]

!pip install tab2img

!pip install sklearn

import sklearn

print(X)
print(X.shape)

print(y)
print(y.shape)

from tab2img.converter import Tab2Img

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

model = Tab2Img()
images = model.fit_transform(X, y)

print(X)
print(X.shape)

print(y)
print(y.shape)

model = Tab2Img()
images = model.fit_transform(X, y)

print(len(images))

from sklearn.model_selection import train_test_split
# split into 67% for train and 33% for test
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)

df_test = pd.read_csv('/content/balanced_data test.csv')
X_test = df_test.iloc[:, :-1].values
y_test = df_test.iloc[:, -1].values

print(X_test.shape), print(y_test.shape)

from pytorch_tabnet.tab_model import TabNetClassifier

tabnet_model = TabNetClassifier()
tabnet_model.fit(
  X_train, y_train,
  eval_set=[(X_valid, y_valid)],
  eval_metric=['accuracy']
)

preds = tabnet_model.predict(X_test)

print(len(preds), preds)
print(len(y_test), y_test)

import matplotlib.pyplot as plt
import numpy as np
import sklearn
from sklearn.metrics import confusion_matrix

plt.plot(tabnet_model.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')

import sklearn
from sklearn import metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
#from scikitplot.metrics import ConfusionMatrixDisplay

classes = ["Malacious","Non-Malacious"]

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve,roc_curve, auc, roc_auc_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import PrecisionRecallDisplay

print(preds.size)

#print confusion matrix
cm = confusion_matrix(y_test,preds)

print('Confusion Matrix')
print(cm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                                display_labels=classes)
fig, ax = plt.subplots(figsize=(5,5))
plt.title("Confusion Matrix")
disp = disp.plot(ax=ax)
plt.show()

#confusion_matrix = confusion_matrix(y_test, preds)
#cm_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
#cm_display.plot()
#plt.show()

Accuracy = metrics.accuracy_score(y_test, preds)
print('Accuracy:', Accuracy)

Precision = metrics.precision_score(y_test, preds)
print('Precision:', Precision)

Recall = metrics.recall_score(y_test, preds)
print('Recall:', Recall)

F1_score = metrics.f1_score(y_test, preds)
print('F1 Score:', F1_score)

False_positive_rate = metrics.confusion_matrix(y_test, preds)[0, 1] / (metrics.confusion_matrix(y_test, preds)[0, 1] + metrics.confusion_matrix(y_test, preds)[0, 0])
print('False Positive Rate:', False_positive_rate)

False_negative_rate = metrics.confusion_matrix(y_test, preds)[1, 0] / (metrics.confusion_matrix(y_test, preds)[1, 0] + metrics.confusion_matrix(y_test, preds)[1, 1])
print('False Negative Rate:', False_negative_rate)

True_positive_rate = metrics.confusion_matrix(y_test, preds)[1, 1] / (metrics.confusion_matrix(y_test, preds)[1, 1] + metrics.confusion_matrix(y_test, preds)[1, 0])
print('True Positive Rate:', True_positive_rate)

True_negative_rate = metrics.confusion_matrix(y_test, preds)[0, 0] /  (metrics.confusion_matrix(y_test, preds)[0, 0] + metrics.confusion_matrix(y_test, preds)[0, 1])
print('True Negative Rate:', True_negative_rate)

Balanced_accuracy = metrics.balanced_accuracy_score(y_test, preds)
print('Balanced Accuracy:', Balanced_accuracy)

TP = metrics.confusion_matrix(y_test, preds)[0, 0]
print('True Positive:', TP)

FP = metrics.confusion_matrix(y_test, preds)[1, 0]
print('False Positive:', FP)

FN = metrics.confusion_matrix(y_test, preds)[0, 1]
print('False Negative:', FN)

TN = metrics.confusion_matrix(y_test, preds)[1, 1]
print('True Negative:', TN)

Accuracy = (TP + TN) / (TP + TN + FP + FN)
print('Accuracy:', Accuracy)

Precision = TP / (TP + FP)
print('Precision:', Precision)
Recall = TP / (TP + FN)
print('Recall:', Recall)

F1_score = 2 * Precision * Recall / (Precision + Recall)
print('F1 Score:', F1_score)

True_negative_rate = TN / (TN + FN)
print('True Negative Rate:', True_negative_rate)

False_negative_rate = FN / (FN + TP)
print('False Negative Rate:', False_negative_rate)

True_positive_rate = TP / (TP + FN)
print('True Positive Rate:', True_positive_rate)

False_positive_rate = FP / (FP + TN)
print('False Positive Rate:', False_positive_rate)

roc_auc = roc_auc_score(y_test,preds)
print('ROC AUC score: %.3f' % roc_auc)
print('Mean ROC AUC: %.5f' % roc_auc.mean())

fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,preds)

auc_keras = auc(fpr_keras, tpr_keras)
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--' ,'No Skill')
plt.plot(fpr_keras, tpr_keras, label = 'area = {:.3f}'.format(auc_keras))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc = 'best')
plt.show()

print(classification_report(y_test, preds))

!pip install torch_geometric

pip install torch_geometric

import os
import torch
import torch.nn.functional as F
from tqdm import tqdm
from torch_geometric.loader import NeighborLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch_geometric.nn import MessagePassing, SAGEConv

"""GNN MODEL"""

import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score

df = pd.read_csv('/content/1subset_10percent train.csv')

# Prepare node features and adjacency matrix
X = df.drop("Class_Label", axis=1).values
num_nodes = X.shape[0]
num_features = X.shape[1]
adjacency = np.eye(num_nodes)  # Assuming each node is connected to itself
features = torch.tensor(X, dtype=torch.float)
edge_index = torch.tensor(np.nonzero(adjacency), dtype=torch.long)

class GNNModel(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GNNModel, self).__init__()
        self.conv1 = GCNConv(num_features, 256)
        self.dropout1 = torch.nn.Dropout(p=0.6)
        self.conv2 = GCNConv(256, num_classes)
        self.dropout2 = torch.nn.Dropout(p=0.6)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout1(x)
        x = self.conv2(x, edge_index)
        x = self.dropout2(x)
        return F.log_softmax(x, dim=1)


X_train, X_valid, y_train, y_valid = train_test_split(X, df["Class_Label"].values, test_size=0.3, random_state=42)
train_features = torch.tensor(X_train, dtype=torch.float)
train_labels = torch.tensor(y_train, dtype=torch.long)
valid_features = torch.tensor(X_valid, dtype=torch.float)
valid_labels = torch.tensor(y_valid, dtype=torch.long)


model_gnn = GNNModel(num_features=num_features, num_classes=2)
optimizer_gnn = torch.optim.Adam(model_gnn.parameters(), lr=0.03)
criterion = torch.nn.CrossEntropyLoss()

def train(model, features, edge_index, labels, optimizer):
    model.train()
    optimizer.zero_grad()
    output = model(features, edge_index)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()
    return loss.item()

def test(model, features, edge_index, labels):
    model.eval()
    with torch.no_grad():
        logits = model(features, edge_index)
        pred = logits.max(dim=1)[1]
        acc = pred.eq(labels).sum().item() / labels.size(0)
    return acc

for epoch in range(300):
    train_loss = train(model_gnn, train_features, edge_index, train_labels, optimizer_gnn)
    train_acc = test(model_gnn, train_features, edge_index, train_labels)
    valid_acc = test(model_gnn, valid_features, edge_index, valid_labels)
    print(f'Epoch: {epoch+1}, Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}, Valid accuracy: {valid_acc:.4f}')

df_test = pd.read_csv('/content/balanced_data test.csv')
X_test = df_test.iloc[:, :-1].values
y_test = df_test.iloc[:, -1].values
test_features = torch.tensor(X_test, dtype=torch.float)
test_labels = torch.tensor(y_test, dtype=torch.long)

test_acc = test(model_gnn, test_features, edge_index, test_labels)
print('Test Accuracy:', test_acc)

model_gnn.eval()

with torch.no_grad():
    predictions = model_gnn(valid_features, edge_index)
    gnn_preds = torch.argmax(predictions, dim=1)
    gnn_acc = accuracy_score(valid_labels, gnn_preds)

print('GNN Accuracy:', gnn_acc)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

model_gnn.eval()

with torch.no_grad():
    predictions = model_gnn(valid_features, edge_index)
    gnn_preds = torch.argmax(predictions, dim=1)

gnn_acc = accuracy_score(valid_labels, gnn_preds)

precision = precision_score(valid_labels, gnn_preds, average='weighted')
recall = recall_score(valid_labels, gnn_preds, average='weighted')
f1 = f1_score(valid_labels, gnn_preds, average='weighted')

conf_matrix = confusion_matrix(valid_labels, gnn_preds)

print(f'GNN Accuracy: {gnn_acc:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print('Confusion Matrix:')
print(conf_matrix)

print(y_test.shape)

model_gnn.eval()

with torch.no_grad():
    # Obtain the raw predictions (logits)
    predictions = model_gnn(valid_features, edge_index)
    # Convert log probabilities to probabilities
    probabilities = torch.exp(predictions)  # since your model outputs log_softmax

if probabilities.shape[1] == 2:
    # Use probabilities of the positive class for ROC AUC
    positive_probs = probabilities[:, 1].cpu().numpy()
    roc_auc = roc_auc_score(valid_labels.cpu().numpy(), positive_probs)
else:
    # For multi-class classification, calculate ROC AUC in a one-vs-rest manner
    roc_auc = roc_auc_score(valid_labels.cpu().numpy(), probabilities.cpu().numpy(), multi_class='ovr')

print(f'ROC AUC Score: {roc_auc:.4f}')

fpr = dict()
tpr = dict()
roc_auc = dict()
if probabilities.shape[1] == 2:  # Binary classification case
    # Use probabilities of the positive class for ROC AUC
    positive_probs = probabilities[:, 1].cpu().numpy()
    fpr[0], tpr[0], _ = roc_curve(valid_labels.cpu().numpy(), positive_probs)
    roc_auc[0] = auc(fpr[0], tpr[0])

    plt.figure()
    plt.plot(fpr[0], tpr[0], color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc[0]:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()
else:  # Multi-class classification case
    valid_labels_one_hot = F.one_hot(valid_labels, num_classes=probabilities.shape[1]).cpu().numpy()
    for i in range(probabilities.shape[1]):
        fpr[i], tpr[i], _ = roc_curve(valid_labels_one_hot[:, i], probabilities[:, i].cpu().numpy())
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot all ROC curves
    plt.figure()
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'navy', 'red', 'green', 'blue']
    for i, color in zip(range(probabilities.shape[1]), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {i} (area = {roc_auc[i]:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic for Multi-Class')
    plt.legend(loc="lower right")
    plt.show()